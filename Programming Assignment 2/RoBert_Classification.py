# -*- coding: utf-8 -*-
"""RoBERT_for_Relation_Classification.ipynb
Automatically generated by Colaboratory.
"""

import torch
import pandas as pd
import numpy as np
import os
from tqdm import tqdm, trange
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.metrics import precision_recall_curve, average_precision_score, PrecisionRecallDisplay, confusion_matrix, classification_report 

from transformers import RobertaConfig, RobertaModel, RobertaTokenizer
from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup
from transformers import BertModel, BertPreTrainedModel

relation_types = ["Component-Whole", "Component-Whole-Inv", "Instrument-Agency", "Instrument-Agency-Inv", "Member-Collection", "Member-Collection-Inv", "Cause-Effect", 
                  "Cause-Effect-Inv", "Entity-Destination", "Entity-Destination-Inv", "Content-Container", "Content-Container-Inv", "Message-Topic", "Message-Topic-Inv",
                  "Product-Producer", "Product-Producer-Inv", "Entity-Origin", "Entity-Origin-Inv", "Other"]

class FCLayer(torch.nn.Module):
  def __init__(self, input_dim, output_dim, dropout_rate=0., use_activation=True):
    super(FCLayer, self).__init__()
    self.use_activation = use_activation
    self.dropout = torch.nn.Dropout(dropout_rate)
    self.linear = torch.nn.Linear(input_dim, output_dim)
    self.tanh = torch.nn.Tanh()
  
  def forward(self, x):
    x = self.dropout(x)
    if self.use_activation:
      x = self.tanh(x)
    return self.linear(x)

class RBERT(BertPreTrainedModel):
  def __init__(self, config, args):
    super(RBERT, self).__init__(config)
    self.roberta = RobertaModel(config=config)
    self.num_labels = config.num_labels
    self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args['DROPOUT_RATE'])
    self.e1_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args['DROPOUT_RATE'])
    self.e2_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args['DROPOUT_RATE'])
    self.label_classifier = FCLayer(config.hidden_size * 3, self.num_labels, args['DROPOUT_RATE'], use_activation=False)

  @staticmethod
  def entity_average(hidden_output, e_mask):
    e_mask_unqueeze = e_mask.unsqueeze(1)
    length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)
    sum_vector = torch.bmm(e_mask_unqueeze.float(), hidden_output).squeeze(1)
    avg_vector = sum_vector.float() / length_tensor.float()
    return avg_vector
  
  def forward(self, input_ids, attention_mask, labels, e1_mask, e2_mask):
    # print("input_ids ", input_ids, " attention_mask ", attention_mask)
    outputs = self.roberta(input_ids, attention_mask=attention_mask)
    sequence_output = outputs[0]
    pooled_output = outputs[1] # cls
    e1_h = self.entity_average(sequence_output, e1_mask)
    e2_h = self.entity_average(sequence_output, e2_mask)

    pooled_output = self.cls_fc_layer(pooled_output)
    e1_h = self.e1_fc_layer(e1_h)
    e2_h = self.e2_fc_layer(e2_h)
    concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)

    logits = self.label_classifier(concat_h)
    outputs = (logits, ) + outputs[2:]

    if labels is not None:
      loss_fct = torch.nn.CrossEntropyLoss()
      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
      outputs = (loss,) + outputs
    return outputs


"""### Dataset Loading"""
train = pd.read_csv('./train.tsv', sep='\t', header=None, names=['label', 'e1','e2', 'text'])
realTrain_df, realDev_df = train_test_split(train, test_size=0.2)
realTest_df = pd.read_csv('./semevalTest_with_keys.tsv', sep='\t', header=None, names=['label', 'e1','e2', 'text'])
sample_train = realTrain_df
sample_dev = realDev_df

"""### Setting Devices as GPU"""
def seed_everything(SEED):
  np.random.seed(SEED)
  torch.manual_seed(SEED)
  torch.cuda.manual_seed(SEED)
  torch.backends.cudnn.deterministic = True
seed_everything(200)

if torch.cuda.is_available():
  device = torch.device("cuda")
#   print("We will use the GPU:", torch.cuda.get_device_name())
else:
  device = torch.device("cpu")
#   print("We will use the CPU.")

"""### Setting Arguments"""
args = {'NUM_LABELS' : 19, 'DROPOUT_RATE' : 0.1, 'LEARNING_RATE' : 2e-5, 'EPOCHS' : 5,
        'MAX_SEQUENCE_LENGTH' : 128, 'BATCH_SIZE' : 10, 'ADAM_EPSILON' : 1e-8,
        'GRADIENT_ACCUMULATION_STEPS' : 1, 'MAX_GRAD_NORM' : 1.0, 'LOGGING_STEPS' : 250,
        'SAVE_STEPS' : 250, 'WEIGHT_DECAY' : 0.0, 'NUM_WARMUP_STEPS' : 0}


"""### Defining Special Tokens and Extend the RobertaTokenizer"""
ADDITIONAL_SPECIAL_TOKENS = ["<e1>", "</e1>", "<e2>", "</e2>"]
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
tokenizer.add_special_tokens({"additional_special_tokens" : ADDITIONAL_SPECIAL_TOKENS})
config = RobertaConfig.from_pretrained('roberta-base', num_labels = args['NUM_LABELS'])
model = RBERT.from_pretrained('roberta-base', config=config, args=args)
model.to(device)

"""### Function that converts data instance into TensorDataset"""
def convert(df, label_indexes, max_seq_len, tokenizer, cls_token='[CLS]', sep_token='[SEP]', pad_token=0, add_sep_token=False, mask_padding_with_zero=True):
  input_ids = []
  attention_masks = []
  e1_masks = []
  e2_masks = []
  labels = []

  for row in df.itertuples():
    ori_text = row.text.split(" ")
    ori_text.insert(row.e2+1, "</e2>")
    ori_text.insert(row.e2, "<e2>")
    ori_text.insert(row.e1+1, "</e1>")
    ori_text.insert(row.e1, "<e1>")
    ori_text = " ".join(ori_text)

    tokens = tokenizer.tokenize(ori_text.lower())
    e11_p = tokens.index("<e1>")
    e12_p = tokens.index("</e1>")
    e21_p = tokens.index("<e2>")
    e22_p = tokens.index("</e2>")
    # Replace token
    tokens[e11_p] = '$'
    tokens[e12_p] = '$'
    tokens[e21_p] = '#'
    tokens[e22_p] = '#'

    # Add 1 because of the [CLS] token
    e11_p += 1
    e12_p += 1
    e21_p += 1
    e22_p += 1

    # Account for [CLS] and [SEP] with "2" and with "3" for RoBERTa
    if add_sep_token:
      special_tokens_count = 2
    else:
      special_tokens_count = 1
    
    if len(tokens) > max_seq_len - special_tokens_count:
      tokens = tokens[:(max_seq_len - special_tokens_count)]
    if add_sep_token:
      tokens += [sep_token]

    tokens = [cls_token] + tokens
    input_id = tokenizer.convert_tokens_to_ids(tokens)
    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_id)

    padding_length = max_seq_len - len(input_id)
    input_id = input_id + ([pad_token] * padding_length)
    attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)

    e1_mask = [0] * len(attention_mask)
    e2_mask = [0] * len(attention_mask)
    for i in range(e11_p, e12_p + 1):
      e1_mask[i] = 1
    for i in range(e21_p, e22_p + 1):
      e2_mask[i] = 1

    assert len(input_id) == max_seq_len, "Error with input length {} vs {}".format(len(input_id), max_seq_len)
    assert len(attention_mask) == max_seq_len, "Error with attention mask length {} vs {}".format(len(attention_mask), max_seq_len)

    input_ids.append(input_id)
    attention_masks.append(attention_mask)
    labels.append(label_indexes.index(row.label))
    e1_masks.append(e1_mask)
    e2_masks.append(e2_mask)
  
  dataset = torch.utils.data.TensorDataset(torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_masks, dtype=torch.long), torch.tensor(labels, dtype=torch.long), 
                                           torch.tensor(e1_masks, dtype=torch.long), torch.tensor(e2_masks, dtype=torch.long))
  return dataset


"""### Process datat"""
sample_train_dataset = convert(sample_train, relation_types, args['MAX_SEQUENCE_LENGTH'], tokenizer, cls_token='<s>', sep_token='</s>', pad_token=1)
sample_train_sampler = torch.utils.data.RandomSampler(sample_train_dataset)
sample_train_loader = torch.utils.data.DataLoader(sample_train_dataset, sampler=sample_train_sampler, batch_size=args['BATCH_SIZE'])

sample_test_dataset = convert(sample_dev, relation_types, args['MAX_SEQUENCE_LENGTH'], tokenizer, cls_token='<s>', sep_token='</s>', pad_token=1)
sample_test_sampler = torch.utils.data.SequentialSampler(sample_test_dataset)
sample_test_loader = torch.utils.data.DataLoader(sample_test_dataset, sampler=sample_test_sampler, batch_size=args['BATCH_SIZE'])

realTest_dataset = convert(realTest_df, relation_types, args['MAX_SEQUENCE_LENGTH'], tokenizer, cls_token='<s>', sep_token='</s>', pad_token=1)
realTest_sampler = torch.utils.data.SequentialSampler(realTest_dataset)
realTest_loader = torch.utils.data.DataLoader(realTest_dataset, sampler=realTest_sampler, batch_size=args['BATCH_SIZE'])


"""### Evaluation and Modeling"""
def evaluate(model, device, test_loader):
    eval_loss = 0.0
    nb_eval_steps = 0
    preds = None
    out_label_ids = None

    model.eval()
    for batch in tqdm(test_loader, desc="Evaluating"):
        batch = tuple(t.to(device) for t in batch)
        with torch.no_grad():
            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2], 'e1_mask': batch[3], 'e2_mask': batch[4]}
            outputs = model(**inputs)
            tmp_eval_loss, logits = outputs[:2]

            eval_loss += tmp_eval_loss.mean().item()
        nb_eval_steps += 1

        if preds is None:
            preds = logits.detach().cpu().numpy()
            out_label_ids = inputs['labels'].detach().cpu().numpy()
        else:
            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)

    eval_loss = eval_loss / nb_eval_steps
    preds = np.argmax(preds, axis=1)

    print(preds, out_label_ids)
    y_true = out_label_ids
    y_pred = preds

    confusion_matrix(y_true, y_pred)

    y_true = label_binarize(y_true, classes=list(range(19)))
    y_pred = label_binarize(y_pred, classes=list(range(19)))

    # For each class
    precision = dict()
    recall = dict()
    average_precision = dict()
    for i in range(19):
        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], y_pred[:, i])
        average_precision[i] = average_precision_score(y_true[:, i], y_pred[:, i])

    precision["micro"], recall["micro"], _ = precision_recall_curve(y_true.ravel(), y_pred.ravel())
    average_precision["micro"] = average_precision_score(y_true, y_pred, average="micro")
    # return np.mean(precision["micro"]), np.mean(recall["micro"]), np.mean(average_precision["micro"])

    result = {'accuracy' : accuracy_score(out_label_ids, preds), 'f1_score': f1_score(out_label_ids, preds, average='macro'), 'y_pred' : preds, 'y_true' : out_label_ids,
              'precision': np.mean(precision["micro"]), 'recall': np.mean(recall["micro"]), 'ap':np.mean(average_precision["micro"])}
    return result


t_total = len(sample_train_loader) // args['GRADIENT_ACCUMULATION_STEPS'] * args['EPOCHS']
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
      'weight_decay': args['WEIGHT_DECAY']},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]
optimizer = AdamW(optimizer_grouped_parameters, lr=args['LEARNING_RATE'], eps=args['ADAM_EPSILON'])
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['NUM_WARMUP_STEPS'], num_training_steps=t_total)

global_step = 0
tr_loss = 0.0
model.zero_grad()
train_iterator = trange(int(args['EPOCHS']), desc="Epoch")
for _ in train_iterator:
    epoch_iterator = tqdm(sample_train_loader, desc="Iteration")
    for step, batch in enumerate(epoch_iterator):
        model.train()
        batch = tuple(t.to(device) for t in batch)  # GPU or CPU
        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2], 'e1_mask': batch[3], 'e2_mask': batch[4]}
        outputs = model(**inputs)
        loss = outputs[0]
        loss.backward()

        tr_loss += loss.item()
        torch.nn.utils.clip_grad_norm_(model.parameters(), args['MAX_GRAD_NORM'])
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        model.zero_grad()

    print("\n====Evaluation====")
    print("\nACCURACY: ", evaluate(model, device, sample_test_loader)[['accuracy', 'f1_score', 'precision', 'recall']])


"""### Evaluation on Test Dataset"""
result = evaluate(model, device, realTest_loader)
print("\precision: {}  \n recall:{} \n Accuracy: {} \n f1-score: {} \n ap: {}".format(result['precision'], result['recall'], result['accuracy'], result['f1_score'], result['ap']))

relation_types.extend(['accuracy', 'macro avg', 'weighted avg'])
report =  classification_report(result['y_true'], result['y_pred'], output_dict=True)
df = pd.DataFrame(report).transpose()
df.index = relation_types
df.sort_values('f1-score', ascending=True)